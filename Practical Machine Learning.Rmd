---
title: "Practical Machine Learning"
author: "Vibhudesh"
date: "November 17, 2019"
output: html_document
---
#Overview
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks.

In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

The data consists of a Training data and a Test data (to be used to validate the selected model).

The goal of your project is to predict the manner in which they did the exercise.
#Data Loading and Processing
```{r , echo=TRUE}
library(caret)
library(e1071)
library(rpart)
library(rpart.plot)
library(rattle)
library(randomForest)
library(corrplot)
library(gbm)
```
#Getting, Cleaning and Exploring the data
```{r , echo=TRUE}
training_in <- read.csv('./pml-training.csv', header=T)
testing_in <- read.csv('./pml-testing.csv', header=T)
dim(training_in)
dim(testing_in)
```
#Cleaning the input data
We remove the variables that contains missing values. 
```{r, echo=TRUE}
trainData<- training_in[, colSums(is.na(training_in)) == 0]
testData <- testing_in[, colSums(is.na(testing_in)) == 0]
dim(trainData)
dim(testData)
```
We now remove the first seven variables as they have little impact on the outcome classe
```{r, echo=TRUE}
trainData <- trainData[, -c(1:7)]
testData <- testData[, -c(1:7)]
dim(trainData)
dim(testData)
```
#Preparing the datasets for prediction
```{r, echo=TRUE}
set.seed(1234) 
inTrain <- createDataPartition(trainData$classe, p = 0.7, list = FALSE)
trainData <- trainData[inTrain, ]
testData <- trainData[-inTrain, ]
dim(trainData)
dim(testData)
```
Cleaning even further by removing the variables that are near-zero-variance
```{r, echo=TRUE}
NZV <- nearZeroVar(trainData)
trainData <- trainData[, -NZV]
testData  <- testData[, -NZV]
dim(trainData)
dim(testData)
```
After this cleaning we are down now to 53 variables
```{r, echo=TRUE}
cor_mat <- cor(trainData[, -53])
corrplot(cor_mat, order = "FPC", method = "color", type = "upper", 
         tl.cex = 0.8, tl.col = rgb(0, 0, 0))
highlyCorrelated = findCorrelation(cor_mat, cutoff=0.75)
names(trainData)[highlyCorrelated]
```
#Model building
For this project we will use two different algorithms, classification trees and random forests, to predict the outcome.

1. classification trees
2. random forests
3. Generalized Boosted Model
#Prediction with classification trees
```{r, echo=TRUE}
set.seed(12345)
decisionTreeMod1 <- rpart(classe ~ ., data=trainData, method="class")
fancyRpartPlot(decisionTreeMod1)
predictTreeMod1 <- predict(decisionTreeMod1, testData, type = "class")
cmtree <- confusionMatrix(predictTreeMod1, testData$classe)
cmtree
plot(cmtree$table, col = cmtree$byClass, 
     main = paste("Decision Tree - Accuracy =", round(cmtree$overall['Accuracy'], 4)))
```
#Prediction with Random Forest
```{r, echo=TRUE}
controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
modRF1 <- train(classe ~ ., data=trainData, method="rf", trControl=controlRF)
modRF1$finalModel
predictRF1 <- predict(modRF1, newdata=testData)
cmrf <- confusionMatrix(predictRF1, testData$classe)
cmrf
plot(modRF1)
plot(cmrf$table, col = cmrf$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(cmrf$overall['Accuracy'], 4)))
```
#Prediction with Generalized Boosted Regression Models
```{r, echo=TRUE}
set.seed(12345)
controlGBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
modGBM  <- train(classe ~ ., data=trainData, method = "gbm", trControl = controlGBM, verbose = FALSE)
modGBM$finalModel
print(modGBM)
predictGBM <- predict(modGBM, newdata=testData)
cmGBM <- confusionMatrix(predictGBM, testData$classe)
cmGBM
```
The accuracy rate using the random forest is very high: Accuracy : 0.9736 and therefore the *out-of-sample-error is equal to 0.0264**.
#Applying the best model to the test data
```{r, echo=TRUE}
Results <- predict(modRF1, newdata=testData)
head(Results)
```